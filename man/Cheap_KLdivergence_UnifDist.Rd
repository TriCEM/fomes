% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/utils.R
\name{Cheap_KLdivergence_UnifDist}
\alias{Cheap_KLdivergence_UnifDist}
\title{Calculate KL Divergence by Forcing Continuous Distribution to Matched Uniform}
\usage{
Cheap_KLdivergence_UnifDist(p, q)
}
\arguments{
\item{p}{probability vector p, considered the empiric, or "data", distribution}

\item{q}{probability vector q, considered the reference, or "model", distribution}
}
\description{
Calculation is a distance between two probability distributions (\emph{N.B.} lacks symmetry).
Assumes use has inputted \emph{R >= Z_0} integer realizations of a test and a model that are drawn
from probability distributions P and Q, respectively. The code then calculates an empiric
distribution for each realization and compares their respective distance.
\emph{NB} Calculation was performed in log space and output metric is in nats (versus log-2 and bits)
}
\details{
This is not a statistically correct test because we force a continuous distribution to
a uniform/discrete state space and drop non-matching overlaps to avoid the issue of log(0)
}
